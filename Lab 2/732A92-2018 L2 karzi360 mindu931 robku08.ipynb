{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2: Information Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Students: mindu931, karzi360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first cell imports the Python module required for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tm2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell imports spaCy and loads its English language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and gold standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is contained in the following file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"/home/TDDE16/labs/l2/data/gmb.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tm2` module defines a function `read_data` that returns an iterator over the lines in a file. You should use this function to read the data for this lab. Use the optional argument `n` to restrict the iteration to the first few lines of the file. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked assailants with grenades and automatic weapons attacked a wedding party in southeastern Turkey, killing 45 people and wounding at least six others.\n",
      "Turkish officials said the attack occurred Monday in the village of Bilge about 600 kilometers from Ankara.\n",
      "The wounded were taken to the hospital in the nearby city of Mardin.\n"
     ]
    }
   ],
   "source": [
    "for sentence in tm2.read_data(data_file, n=3):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the raw data, we also provide you with a gold standard of entity pairs that your system should be able to extract. The following code loads these pairs from the file `gold.txt` and adds them to the set `gold`. Each pair is augmented with the identifier of the sentence (line number in the data file) which it was extracted from. Note that the sentence (line) numbering starts at index&nbsp;0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "802\tAli Zardari\tPakistan People 's Party\n",
      "\n",
      "2297\tAbdul Aziz al-Hakim\tSupreme Council\n",
      "\n",
      "4823\tSlavkov\tBulgarian National Olympic Committee\n",
      "\n",
      "7902\tMr. Hakim\tSupreme Council\n",
      "\n",
      "8206\tJ. Patrick Boyle\tAmerican Meat Institute\n",
      "\n",
      "8633\tAli Rodriguez\tPetroleos de Venezuela\n",
      "\n",
      "9004\tForeign Minister Joschka Fischer\tGreen Party\n",
      "\n",
      "11021\tKhalaf\tal-Qaida\n",
      "\n",
      "11259\tJoseph Domenech\tU.N. 's Food and Agricultural Organization\n",
      "\n",
      "13043\tDavid Petraeus\tU.S. Central Command\n",
      "\n",
      "15203\tJoseph Kony\tLord 's Resistance Army\n",
      "\n",
      "15494\tKhodorkovsky\tYukos\n",
      "\n",
      "15906\tPresident Chen Shui-bian\tDemocratic Progressive Party\n",
      "\n",
      "18977\tGeneral Petraeus\tU.S. Central Command\n",
      "\n",
      "20496\tAvigdor Lieberman\tYisrael Beitenu\n",
      "\n",
      "20667\tMr. Fini\tNational Alliance\n",
      "\n",
      "21914\tMr. Mwanawasa\tSouthern African Development Community\n",
      "\n",
      "23016\tOsama bin Laden\tal-Qaida\n",
      "\n",
      "28997\tMa\tNationalist Party\n",
      "\n",
      "30171\tal-Zarqawi\tal-Qaida\n",
      "\n",
      "31546\tMr. Abbas\tFatah\n",
      "\n",
      "32262\tMorgan Tsvangirai\tMovement for Democratic Change\n",
      "\n",
      "33646\tMr. Coleman\tSenate Government Affairs\n",
      "\n",
      "34889\tPrince Ali\tWest Asian Football Federation\n",
      "\n",
      "36946\tVintsuk Vyachorka\tBelarus Popular Front\n",
      "\n",
      "37037\tAli Akbar Salehi\tAtomic Energy Organization\n",
      "\n",
      "40736\tLal Krishna Advani\tBharatiya Janata Party\n",
      "\n",
      "41998\tEbadi\tHuman Rights Center\n",
      "\n",
      "42098\tMr. Abbas\tFatah\n",
      "\n",
      "44280\tGandhi\tNational Advisory Council\n",
      "\n",
      "44637\tSaad al-Fagih\tMovement of Islamic Reform\n",
      "\n",
      "44784\tMr. Rafsanjani\tExpediency Council\n",
      "\n",
      "44908\tKevin Costner\tKevin Costner Band\n",
      "\n",
      "48378\tAbu Musab al-Zarqawi\tal Qaida\n",
      "\n",
      "49242\tAyatollah Ahmad Jannati\tGuardian Council\n",
      "\n",
      "49336\tManie de Clerq\tPublic Servants Association\n",
      "\n",
      "49705\tOcalan\tKurdistan Workers Party\n",
      "\n",
      "51157\tAbu Musab al-Zarqawi\tal-Qaeda\n",
      "\n",
      "51507\tAbdullah Ocalan\tKurdistan Workers Party\n",
      "\n",
      "51967\tSaad al-Fagih\tMovement for Islamic Reform\n",
      "\n",
      "53075\tMr. Rafsanjani\tExpediency Council\n",
      "\n",
      "57350\tGene Sperling\tNational Economic Council\n",
      "\n",
      "57420\tMajor General Udi Adam\tNorthern Command\n",
      "\n",
      "60729\tGeneral David Petraeus\tU.S. Central Command\n",
      "\n",
      "61152\tSaad al-Fagih\tMovement for Islamic Reform\n",
      "\n",
      "61337\tLisa Jackson\tEnvironmental Protection Agency\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gold_file = \"/home/TDDE16/labs/l2/data/gold.txt\"\n",
    "\n",
    "gold = set()\n",
    "with open(gold_file) as fp:\n",
    "    for line in fp:\n",
    "        print(line)\n",
    "        columns = line.rstrip().split('\\t')\n",
    "        gold.add((int(columns[0]), columns[1], columns[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code prints the 10&nbsp;first pairs from the gold standard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "802\tAli Zardari\tPakistan People 's Party\n",
      "2297\tAbdul Aziz al-Hakim\tSupreme Council\n",
      "4823\tSlavkov\tBulgarian National Olympic Committee\n",
      "7902\tMr. Hakim\tSupreme Council\n",
      "8206\tJ. Patrick Boyle\tAmerican Meat Institute\n",
      "8633\tAli Rodriguez\tPetroleos de Venezuela\n",
      "9004\tForeign Minister Joschka Fischer\tGreen Party\n",
      "11021\tKhalaf\tal-Qaida\n",
      "11259\tJoseph Domenech\tU.N. 's Food and Agricultural Organization\n",
      "13043\tDavid Petraeus\tU.S. Central Command\n"
     ]
    }
   ],
   "source": [
    "for i, person, org in sorted(gold)[:10]:\n",
    "    print(\"{}\\t{}\\t{}\".format(i, person, org))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the entity extraction part of our system, we use the full natural language processing power built into spaCy. The following code extracts the entities from the first 5&nbsp;sentences of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turkey\t13\t14\tGPE\n",
      "45\t16\t17\tCARDINAL\n",
      "at least six\t20\t23\tCARDINAL\n",
      "Turkish\t0\t1\tNORP\n",
      "Monday\t6\t7\tDATE\n",
      "Bilge\t11\t12\tORG\n",
      "about 600 kilometers\t12\t15\tQUANTITY\n",
      "Ankara\t16\t17\tGPE\n",
      "Mardin\t12\t13\tORG\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(nlp.pipe(tm2.read_data(data_file, n=5))):\n",
    "    for ent in doc.ents:\n",
    "        print(\"{}\\t{}\\t{}\\t{}\".format(ent.text, ent.start, ent.end, ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Extract relevant pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identify pairs of entities that are in the &lsquo;is-leader-of&rsquo; relation, based on the strategy outlined in the section on [Relation Extraction](http://www.nltk.org/book/ch07.html#relation-extraction) in the book by Bird, Klein, and Loper (2009):\n",
    "\n",
    "* look for all triples of the form $(x, \\alpha, y)$ where $x$ and $y$ denote named entities of type *person* and *organisation*, respectively, and $\\alpha$ is the intervening text\n",
    "* write a regular expression to match just those instances of $\\alpha$ that express the &lsquo;is-leader-of&rsquo; relation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract(doc):\n",
    "    \"\"\"Extract relevant relation instances from the specified document.\n",
    "    \n",
    "    Args:\n",
    "        doc: The sentence as analysed by spaCy.\n",
    "    Yields:\n",
    "        Pairs of strings representing the extracted relation instances.\n",
    "    \"\"\"  \n",
    "    #define the relationship\n",
    "    rela = re.compile(r'.*(lead|head|chief|preside|minist|command|manage|supervis|rule|direct).*')                 \n",
    "    \n",
    "    #matches list\n",
    "    matches = []\n",
    "    #Scan all lines\n",
    "    total_rows = range(len(doc.ents) -1)\n",
    "    for n in total_rows:\n",
    "   \n",
    "        e1 = doc.ents[n]\n",
    "        e2 = doc.ents[n+1]\n",
    "        \n",
    "        #Only look for entity 1 is person and entity 2 is org\n",
    "        if (not e1.label_ == \"PERSON\") or (not e2.label_ in \"ORG\"):\n",
    "            continue\n",
    "        \n",
    "        #get the text and compare\n",
    "        txt = doc[e1.end:e2.start]\n",
    "        is_match = rela.match(str(txt))\n",
    "        if is_match:\n",
    "            matches.append((str(e1), str(e2)))\n",
    "            \n",
    "    \n",
    "    return matches \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows how your function is supposed to be used. The code prints out the extracted pairs for the first 1,000&nbsp;sentences in the data. It additionally numbers each pair with the sentence identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207\tRugova\tEuropean Union\n",
      "351\tJendayi Frazer\tSudan Liberation Army\n",
      "512\tAung San Suu Kyi\tthe National League for Democracy\n",
      "736\tViktor Yanukovych\tRussian Party\n",
      "802\tAsif Ali Zardari\tthe Pakistan People's Party\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(nlp.pipe(tm2.read_data(data_file, n=1000))):\n",
    "    for person, org in extract(doc):\n",
    "        print(\"{}\\t{}\\t{}\".format(i, person, org))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell below will process all lines of data file (62k sentences). Then, add all extracted pairs to the `extracted` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 62010 sentences ... done\n"
     ]
    }
   ],
   "source": [
    "extracted = set()\n",
    "for i, doc in enumerate(nlp.pipe(tm2.read_data(data_file))):\n",
    "    for person, org in extract(doc):\n",
    "        extracted.add((i, person, org))\n",
    "    print('\\rProcessed {} sentences ...'.format(i+1), end='', flush=True)\n",
    "print(' done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing the above cell, all extracted id-string-string triples are in the set `extracted`. The code in the next cell will print the first 10&nbsp;triples in this set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207\tRugova\tEuropean Union\n",
      "351\tJendayi Frazer\tSudan Liberation Army\n",
      "512\tAung San Suu Kyi\tthe National League for Democracy\n",
      "736\tViktor Yanukovych\tRussian Party\n",
      "802\tAsif Ali Zardari\tthe Pakistan People's Party\n",
      "1349\tKaren Hughes\tState Department\n",
      "1790\tKoizumi\tthe United Nations\n",
      "2297\tAbdul Aziz al-Hakim\tthe Supreme Council for the Islamic Revolution in Iraq\n",
      "3274\tJack Abramoff\tCongress\n",
      "3291\tKrasniqi\tthe Kosovo Protection Corps\n",
      "Length of the extracted set: 192\n",
      "Length of the gold set: 46\n"
     ]
    }
   ],
   "source": [
    "for i, person, org in sorted(extracted)[:10]:\n",
    "    print(\"{}\\t{}\\t{}\".format(i, person, org))\n",
    "print(\"Length of the extracted set:\", len(extracted))\n",
    "print(\"Length of the gold set:\", len(gold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the length of the `extracted` set and the `gold` set below. We can see that the length of the `extracted` set is 4 times higher. Let's continue with others problem and see what happend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Evaluate your system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cells below will compute the precision, recall, and F1 measure of your extractor relative to the gold standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(reference, predicted):\n",
    "    \"\"\"Print out the precision, recall, and F1 for the id-entity-entity\n",
    "    triples in the set `predicted`, given the triples in the reference set.\n",
    "    \n",
    "    Args:\n",
    "        reference: The reference set of triples.\n",
    "        predicted: The set of predicted triples.\n",
    "    Returns:\n",
    "        Nothing, but prints out precision, recall, and F1.\n",
    "    \"\"\"\n",
    "    # false negatives:  data not in predicted but is in reference\n",
    "    # false positives: data not in reference but \n",
    "    # true positive: data in both predicted and reference \n",
    "    tp = predicted.intersection(reference)\n",
    "    \n",
    "    # true positive + false positive = predicted\n",
    "    # true positive + false negative = reference\n",
    "    precision = len(tp)/len(predicted)\n",
    "    recall    = len(tp)/len(reference)\n",
    "    F1        = (2*precision*recall)/(precision + recall)\n",
    "  \n",
    "    print(\"Percision:\", round(precision*100,2), \"%\")\n",
    "    print(\"Recall:\", round(recall*100,2), \"%\")\n",
    "    print(\"F1:\", round(F1*100,2), \"%\")\n",
    "\n",
    "#print(tm2.evaluate(gold, extracted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the reults. The low result is predicted because the higher different in the length of 2 sets as mentioned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percision: 2.6 %\n",
      "Recall: 10.87 %\n",
      "F1: 4.2 %\n"
     ]
    }
   ],
   "source": [
    "evaluate(gold, extracted)\n",
    "#print(tm2.evaluate(gold, extracted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Entity resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realise that your extractor (probably) does a rather poor job in matching the gold standard. One reason for this is that the NLP preprocessing is not perfect (spaCy was not trained on the annotations in the Groningen Meaning Bank), and that the approach of using regular expressions for relation extraction is rather naive.\n",
    "\n",
    "Another reason however is that the current version of your system does not include a component for *entity resolution*. To give an example, your system does not realise that the strings `David Petraeus` and `General David Petraeus` refer to the same entity.\n",
    "\n",
    "To solve the problem, we implement a function called `normalise` that takes an entity mention (a string) as its input and rewrites it to the form used in the gold standard. While this is &lsquo;cheating&rsquo;, it allows we to assess the performance of a more realistic system, and helps to illustrate that information extraction can be very domain-specific.\n",
    "\n",
    "The following cell contains skeleton code for the `normalise` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(text):\n",
    "    #Scan the gold files, if some words is match - return the entity\n",
    "    for (i,e1,e2) in gold:\n",
    "        if (text in e1) or (e1 in text):\n",
    "            return e1\n",
    "        elif (text in e2) or (e2 in text):\n",
    "            return e2\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell shows how `normalise` is intended to be used. Each triple in the set `extracted` is transformed by feeding the two entity mentions into the `normalise` function. The normalised triples are then added to a new set `extracted_normalised`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_normalised = set()\n",
    "for triple in extracted:\n",
    "    extracted_normalised.add((triple[0], normalise(triple[1]), normalise(triple[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the new results, after using normalisation rules. We can see that it is much more better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percision: 13.54 %\n",
      "Recall: 56.52 %\n",
      "F1: 21.85 %\n"
     ]
    }
   ],
   "source": [
    "evaluate(gold, extracted_normalised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we do the error analysis of our information extraction system. We will do it by hand, and then use the visualisation tools provided by spaCy. \n",
    "For example, the following code cell visualises the output of the named entity recogniser for the given input sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Slavkov\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " will lose his position as head of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    the Bulgarian National Olympic Committee\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "sentence = u'Slavkov will lose his position as head of the Bulgarian National Olympic Committee.'\n",
    "\n",
    "displacy.render(nlp(sentence), style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Recall-related errors (false negatives)\n",
    "\n",
    "Requirement: \"By tuning the `normalise` function, you can deal with some of the recall-related mistakes that your system makes. Other recall-related errors cannot be fixed in this way. To illustrate this, find at least 5&nbsp;entity pairs in the gold standard that your system still does not identify correctly, and enter them into the text box below. For each example, provide a brief explanation of what goes wrong. Try to find examples that illustrate different types of errors.\"\n",
    "\n",
    "Here is the 5 false negatives entities"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sentence_id    entity 1          entity 2\n",
    "    4823       Slavkov\t         Bulgarian National Olympic Committee\n",
    "    7902\t   Mr. Hakim         Supreme Council\n",
    "    11021\t   Khalaf\t         al-Qaida\n",
    "    15494\t   Khodorkovsky\t     Yukos\n",
    "    18977\t   General Petraeus\t U.S. Central Command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, here is the full text of these sentences:\n",
    "\n",
    "4823: Slavkov will lose his position as head of the Bulgarian National Olympic Committee.  \n",
    "7902: Mr. Hakim heads the Shi'ite dominated Supreme Council for the Revolution in Iraq, which has the largest representation in parliament.    \n",
    "11021: According to the department, Khalaf is a senior leader of al-Qaida in Iraq's \"facilitation network,\" which controls the flow of resources -- including weapons, money and militants -- from Syria into Iraq.  \n",
    "15494: Thursday, a Moscow court rejected Khodorkovsky's appeal of his conviction, but supporters of the former head of the oil firm Yukos say authorities rushed through the process to prevent him from running in a December parliamentary byelection.  \n",
    "18977: Lawmakers voted 95 - 2 to make General Petraeus the new leader of the U.S. Central Command, which oversees American forces in the Middle East, East Africa and Central Asia.  \n",
    "\n",
    "\n",
    "As we can see, these sentence is quite complicated. it not in the format like `Person` rela `ORG`. So, our function failed to detect. To be specific, please look at the example below. The tool is wrong when tag the entity label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">Mr. \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Hakim\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " heads the \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Shiite\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " dominated \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Supreme Council for the Revolution in Iraq\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", which has the largest representation in parliament.   </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence7902 = u'Mr. Hakim heads the Shiite dominated Supreme Council for the Revolution in Iraq, which has the largest representation in parliament.   '\n",
    "displacy.render(nlp(sentence7902), style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-related errors (false positives)\n",
    "\n",
    "Next, provide at least 5 entity pairs that represent false positives of your system. Explain what goes wrong."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sentence_id    entity 1             entity 2\n",
    "    207\t       Rugova\t            European Union\n",
    "    351\t       Jendayi Frazer\t    Sudan Liberation Army\n",
    "    512\t       Aung San Suu Kyi\t    the National League for Democracy\n",
    "    736\t       Viktor Yanukovych\tRussian Party\n",
    "    1349\t   Karen Hughes\tState   Department"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these pairs is still true. But, some of it false because the sentences is quite complicated. For example, in the first sentance: Rugova will meet with the leader of European Union. (He is the leader of a difference oganization, but not the EU).  In the second ones. Jendayi Frazer will meet the leader of Sudan Liberation Army, but he is not the leader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">The blast occurred at \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    8:20 am\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
       "</mark>\n",
       " as President \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Rugova s\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " motorcade was headed to a meeting with \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    European Union\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " foreign policy chief \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Javier Solana\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ".  </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">\n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Saturday\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", a senior \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    U.S.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " official, Assistant Secretary of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    State\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " for African Affairs, \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Jendayi Frazer\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", met with rival leaders of the rebel \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Sudan Liberation Army\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " to urge them to present a united front at the next round of peace talks.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence207 = u'The blast occurred at 8:20 am as President Rugova s motorcade was headed to a meeting with European Union foreign policy chief Javier Solana.  '\n",
    "displacy.render(nlp(sentence207), style='ent', jupyter=True)\n",
    "sentence351 = u'Saturday, a senior U.S. official, Assistant Secretary of State for African Affairs, Jendayi Frazer, met with rival leaders of the rebel Sudan Liberation Army to urge them to present a united front at the next round of peace talks.'\n",
    "displacy.render(nlp(sentence351), style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incompleteness of the gold standard\n",
    "\n",
    "You may have noticed that some of your system&rsquo;s false positives are actually &lsquo;correct&rsquo;. This can happen because, while each entity pair in the gold standard has been manually checked for correctness, no check has been made that the gold standard contains all relevant pairs. Find at least 5&nbsp;entity pairs in the data that are valid instances of the &lsquo;is-leader-of&rsquo; relation (according to your subjective judgement) but that are not contained in the gold standard."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sentence_id    entity 1             entity 2\n",
    "    512\t       Aung San Suu Kyi\t    the National League for Democracy\n",
    "    736\t       Viktor Yanukovych\tRussian Party\n",
    "    1790\t   Koizumi\t            the United Nations\n",
    "    2297\t   Abdul Aziz al-Hakim\tthe Supreme Council for the Islamic Revolution in Iraq\n",
    "    3291\t   Krasniqi\t            the Kosovo Protection Corps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">At the time of his arrest, Mr. \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Krasniqi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " was a commander of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    the Kosovo Protection Corps\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", a post-war civil defense group that deals with emergencies in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Kosovo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence3291= \"At the time of his arrest, Mr. Krasniqi was a commander of the Kosovo Protection Corps, a post-war civil defense group that deals with emergencies in Kosovo.\"\n",
    "displacy.render(nlp(sentence3291), style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Did you find any examples that you did not find when looking for false positives?\"   \n",
    "\n",
    "Yes. For example, this is the sentence number 10: \"In a report issued Tuesday, U.N. Secretary-General Kofi Annan says Haiti is at a critical juncture, as the country prepares for its first set of elections since the ouster of President Jean-Bertrand Aristide in February, 2004.\"\n",
    "\n",
    "In this sentence, Jean-Bertrand Aristide is the president (~ leader) of Haiti. But this sentence doesn't appear in both `gold` and `extracted` set. And, again, this sentence is quite complicated. It's hard for extract the relationship in a sentence like this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of the assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
